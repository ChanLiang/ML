# 一. 模型实现
## 1. 原理
原理很简单，`有监督学习`，没有显式的训练过程，`分类和回归`都可以用。<br>
以`分类任务`为例，新来了一个样本，像判断它的类别，那么就依次计算它和每一个训练样本的距离，选取k个与它最接近的训练样本，看看里边什么类别最多，那就把这个样本预测成什么类别，简单粗暴。<br><br>
## 2. 代码
写代码的话，我觉得需要注意2点就可以：<br>
*  一是在算新样本和所有训练样本的距离时，不要用for循环去遍历(O(m))，尽量用numpy的`矩阵运算`，这样可以提速（利用cpu/gpu的并行处理能力——单指令多数据）<br>
*  二是python中，dict的排序和遍历都需要用dict.items()转化成tupple list在做(好吧，这个是说给我自己的...)<br><br>
## 3. 优化
至于在时间上进一步优化，主要在于如何才能快速的找出k个最近邻:
* 要么`brute蛮力搜索`O(m)，虽然是线性时间复杂度，数据量大了还是会很慢<br>
* 要么使用高级的数据结构O(logm)——`kd-tree`(就是二叉树,以中值切分构造的树，每个结点是一个超矩形，可参照李航老师的《统计学习方法》)，`ball-tree`(为了克服kd树高纬失效而发明的，其构造过程是以质心C和半径r分割样本空间，每个节点是一个超球体)<br><br>
基本实现就是这么多了，下面会用kNN来做一些简单的应用。
